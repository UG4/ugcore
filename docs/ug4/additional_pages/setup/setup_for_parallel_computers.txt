//	created by Martin Rupp
//	martin.rupp@gcsc.uni-frankfurt.de
//	y12 m02 d29

/**	\page pageUG4SetupParallel Setting up UG on parallel computers / clusters

	- \ref secGeneral_Notes "General Notes"
	- \ref secCMake "CMake, Toolchains, Compilers"
	- \ref secInstallation_of_additional_software "Installation of additional software"
	- \ref secMacOSX "MacOSX"
	- \ref secCekon	"Cekon"
	- \ref secJuGene "JuGene"
	- \ref secHermit "Hermit"
	- \ref secSSH "SSH" (\ref secSSHKeys, \ref secSSHHoping, \ref secSSHFS) 

<hr>
\section secGeneral_Notes General Notes
<hr>
All examples are for running ug in parallel with <tt>&lt;NP&gt;</tt> processors and <tt>$UGARGS</tt> as arguments, where
<tt>&lt;NP&gt;</tt> is a placeholder for the number of MPI processes and
<tt>$UGARGS</tt> is an Unix environment variable which for example is defined by (Bash syntax; just to shorten command lines):

\verbatim
UGARGS="-ex ../scripts/tests/modular_scalability_test.lua -dim 2 -grid ../data/grids/unit_square_01/unit_square_01_quads_8x8.ugx"
\endverbatim

Except for your own computer/workstation or explicitly stated, do NOT EVER use <tt>mpirun -np &lt;NP&gt; ugshell $UGARGS</tt>
to start your job on a cluster! The node you are logging into is only a login node, and you don't 
want to run your job on these.

<hr>
\section secCMake CMake, Toolchains, Compilers
<hr>
\subsection secCMake_ToolchainFiles Toolchain file

On some systems (especially when the software is built for a different system than the one which does the build)
it is necessary to change some configuration settings done by CMake (like compilers or flags to use)
by a so called "toolchain file"
(cf. for example <a href="http://www.vtk.org/Wiki/CMake_Cross_Compiling">CMake Cross Compiling</a>).

In this case run CMake like this
\verbatim
cmake -DCMAKE_TOOLCHAIN_FILE=<TOOLCHAINFILE> ..
\endverbatim

\subsection secCMake_OtherCompilers Setting compilers to use

You can specify other compilers than detected by CMake from the command line with
\verbatim
cmake -DCMAKE_C_COMPILER=cc -DCMAKE_CXX_COMPILER=CC ..
\endverbatim

\subsection secCMake_GCC412 GCC 4.1.2

GCC v. 4.1.2, like it is default on cekon, is not able to compile a debug build
(i.e. configured with <tt>cmake -DDEBUG=ON ..</tt>) because of an internal compiler error
(<tt>internal compiler error: in force_type_die, at dwarf2out.c...</tt>).
In this case it is possible to configure ug4 by specifying GCC v. 4.4.4 which is also installed on cekon as an alternative compiler:
\verbatim
cmake -DCMAKE_CXX_COMPILER=/usr/bin/g++44 ..
\endverbatim

Alternatively one can instruct the (default) compiler to produce the debug infos in <a href="http://gcc.gnu.org/ml/gcc/2001-04/msg01028.html">another format</a>. 
To do so, call cmake with
\verbatim
cmake -DDEBUG_FORMAT=-gstabs ..
\endverbatim
This sets STABS as format of debug infos.
<br>
<hr>
If you need to choose another compiler, please consider writing your own toolchain file, so others can benefit from your knowledge. 

<hr>
\section secInstallation_of_additional_software Installation of additional software
<hr>

Unfortunately on some systems it turned out that especially the build tool CMake,
absolutely necessary to configure ug4 (cf. \ref pageInstallUG4, \ref secInstallUG4CMake),
was not available. In such cases you have to install the required software yourself
(typically locally). For some installation instructions - including those for CMake -
see \ref pageAdditionalSoftware .

<hr>
\section secWindows Windows
<hr>


<hr>
\section secMacOSX MacOSX
<hr>
After installation of a MPI implementation (e.g. <a href="http://www.open-mpi.org/">OpenMPI</a>,
for example via <a href="http://www.macports.org">MacPorts</a>: <tt>sudo port install openmpi</tt>)
you can use
\verbatim
mpirun -np <NP> ugshell $UGARGS
\endverbatim
to run ug4.


<hr>
\section secCekon Cekon
<hr>
Cekon is the in-house cluster of the G-CSC.
By now it consists of 23 compute nodes with 4 cores per node, that is 92 computing cores.

<ul>
<li> Configuration: Normally a run of CMake with "standard" parameters should do the job.

    There are some problems with the pre-installed GCC 4.1.2, see \ref secCMake_GCC412 .
    You can also use <tt>icc</tt> as compiler.
</li>

The Job Scheduler on Cekon is supported by ugsubmit/uginfo/ugcancel (\ref pageugsubmit).<br>

If you want to do job scheduling manually:
<li> Start your job with
    \verbatim
    salloc -n <NP> mpirun ugshell $UGARGS
    \endverbatim
    Please note that ony the <tt>salloc</tt> parameter <tt>-n</tt> reserves a number of processes / cores of a job,
    while <tt>-N</tt> (capital N) a number of nodes. See the <tt>salloc</tt> manual page for further details
</li>

<li>To display information about jobs already running (located in the SLURM scheduling queue) use the <tt>squeue</tt> command.
</li>

<li> <b>Debugging</b>: If DDT is installed, simply type <tt>ddt</tt> in the Unix shell
     to start the Debugger and fill in the job definition of the job to be debugged in the GUI (X11 based) --
     everything should be quite self-explanatory.
</li>
</ul>

<hr>
\section secJuGene JuGene
<hr>

<a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUGENE/JUGENE_node.html">JuGene</a> - the 
<a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUGENE/Configuration/Configuration_node.html">72 racks Blue Gene/P </a> system at J&uuml;lich Supercomputing Centre (JSC)
(FZ J&uuml;lich) in total provides 
294.912 cores (288 Ki) and 144 Tbyte RAM. The 73.728 compute nodes (CN) each have a quadcore Power 32-bit PC 450 running at 850 MHz, with 2 Gbyte of RAM. 

Half a rack (2048 cores) is called a <b>midplane</b>.
The JuGene system uses five different networks dedicated for various tasks and functionalities of the machine.
Relevant for us: The 3-D torus network. This is a point-to-point network - each of the CNs has six nearest-neighbour connections.


See more on the architecture of JuGene <a href="http://www.fz-juelich.de/SharedDocs/Downloads/IAS/JSC/EN/JUGENE/SlidesBGPArchitecture.pdf"> here </a>.

More information about JuGene "in order to enable users of the system to achieve good performance of their applications" can be found in
<a href="http://www.prace-ri.eu/IMG/pdf/Best-practise-guide-JUGENE-v0-3.pdf">"PRACE Best-Practice Guide for JUGENE"</a>

<p>
Note that the login nodes are running under "SuSE Linux Enterprise Server 10" (SLES 10), while the CNs are running a limited version of Linux called "Compute Node Kernel" (CNK),
Therefore its necessary to <b>cross-compile</b> for JuGene (cf. sec. \ref secCMake; sec. \ref secConfiguration_of_ug4_for_JuGene).

<hr>
\subsection secAccess_to_JuGenes_login_nodes Access to JuGene's login nodes:
<hr>
JuGene is reached via two so called <b>front-end</b> or <b>login nodes</b> ('jugene3' and 'jugene4')
for interactive access and the submission of batch jobs.

These login nodes are reached via
\verbatim
ssh <user>@jugene.fz-juelich.de
\endverbatim
i.e., for login there is only a generic hostname, <tt>jugene.fz-juelich.de</tt>,
from which automatically a connection either to <tt>jugene3</tt> or <tt>jugene4</tt> will be established.

The front-end nodes have an identical environment, but multiple sessions of one user may reside on different nodes which must be taken into account when killing processes.

It is necessary to <b>upload the SSH key</b> of the machine from which to connect to one of JuGenes login nodes.
See <a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUGENE/UserInfo/Logon.html">Logging on to JUGENE</a> (also for X11 problems).

To be able to connect to JuGene from different machines maybe you find it useful to define one of GCSCs machines
(e.g. speedo, quadruped, ...) as a "springboard" to one of JuGenes login nodes (so that you have to login to this
machine first, then to JuGene), see \ref secSSHHoping.

<hr>
\subsection secConfiguration_of_ug4_for_JuGene Configuration of ug4 for JuGene
<hr>

For JuGene you have to "<b>cross compile</b>" and to do so use a specific \ref secCMake_ToolchainFiles. Start CMake like this
\verbatim
cmake -DCMAKE_TOOLCHAIN_FILE=../toolchain_file_jugene.cmake ..
\endverbatim
or, for <b>static builds</b> which is the <b>configuration of joice</b> if you want to <b>process very large jobs</b>,

\verbatim
cmake -DSTATIC=ON -DCMAKE_TOOLCHAIN_FILE=../toolchain_file_jugene_static.cmake ..
\endverbatim

See also \ref secVery_large_jobs_on_JuGene!

<b>Note</b>: A "static build" where also all system libraries are linked statically need some additional "hand work" by now:
After configuration with CMake edit the following files by replacing all occurences of <tt>libXXXXXXX.so</tt> by <tt>libXXXXXXX.a</tt>
(has to be done only once):
\verbatim
   CMakeCache.txt,                                                                                                                                
   ugbase/ug_shell/CMakeFiles/ugshell.dir/link.txt,                                                                                               
   ugbase/ug_shell/CMakeFiles/ugshell.dir/build.make                                                                                              
\endverbatim

Or use this <tt>sed</tt> command:
\verbatim
sed -i '' 's/\([[:alnum:]]*\).so/\1.a/g' CMakeCache.txt ugbase/ug_shell/CMakeFiles/ugshell.dir/link.txt ugbase/ug_shell/CMakeFiles/ugshell.dir/build.make 
\endverbatim

You can check your executable by running the <tt>ldd</tt> command on it:
\verbatim
ldd ugshell
\endverbatim
Answer should be <tt>not a dynamic executable</tt> for a completely static build!

<hr>
\subsection secWorking_with_ug4_on_JuGene Working with ug4 on JuGene
<hr>

<hr>
\subsubsection secBasic_job_handling Basic job handling
<hr>
Please take the time and fill out the details in ugsubmit/uginfo/ugcancel (\ref pageugsubmit).
<hr>
See <a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUGENE/UserInfo/QuickIntroduction.html">Quick Introduction</a> to job handling.
Also look at <a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUGENE/UserInfo/mpirun.html">job/mpirun options</a>.

Here we only introduce some important details (everything else should become clear by examining the examples provided):
<ul>
<li> <b>Jobs can be submitted</b> by the <tt>llrun</tt> (interactively) and the <tt>mpirun</tt> (as batch job; defined in a LoadLeveler script) command.
     See below for some details.
</li>

<li> There are different <b>execution modes</b> which can be specified by the <tt>mpirun</tt> and <tt>llrun</tt> parameter
     <tt>-mode {VN | DUAL | SMP}</tt>:

     <ul>
     <li> <b>Quad Mode</b> (a.k.a. "Virtual Mode"):
	All four cores run one MPI process. Memory/MPI Process = 1/4 CN RAM: <tt>-mode VN</tt>.
     </li>

     <li> <b>Dual Mode</b>:
	Two cores run one MPI process (hybrid MPI/OpenMP). Memory/MPI Process = 1/2 CN RAM: <tt>-mode DUAL</tt>.
     </li>

     <li> <b>SMP Mode</b> ("Symmetrical Multi Processing Mode"):
	All four cores run one MPI process (hybrid MPI/OpenMP). Memory/MPI Process = CN RAM: <tt>-mode SMP</tt>.
     </li>
     </ul>
     Note that in quad mode (using all 4 processors of a computing node) this means each core has only ca. 512 Mbyte of RAM
     (474 Mbyte to be more specific, since the CNK also needs some memory).
     <p>

     Obviously "VN" is the preferred execution mode if large numbers of processes should be achieved -
     and ug4 works with VN mode (at least up to ~64 Ki DoFs per process)!
</li>

<li> The <tt>mpirun</tt> parameter <b><tt>-mapfile</tt></b> specifies an order in which the MPI processes are mapped to the CNs / the cores of the BG/P partition reserved for the run.
     This order can either be specified by a permutation of the letters X,Y,Z and T <i>or</i> the name of a mapfile in which the distribution of the tasks is specified,
     <tt>-mapfile {&lt;mapping&gt;|&lt;mapfile&gt;}</tt>:

     <ul>
     <li> <tt>&lt;mapping&gt;</tt> is a permutation of X, Y, Z and T.

     	  The standard mapping on JuGene is to place the tasks in "XYZT" order,
	  where X, Y, and Z are the torus coordinates of the nodes in the partition and T is the number of the cores within each node (T=0,1,2,3).

	  When the tasks are distributed across the nodes the first dimension is increased first,
	  i.e. for XYZT the first three tasks would be executed by the nodes with the torus coordinates
	  <tt>&lt;0,0,0,0&gt;</tt>, <tt>&lt;1,0,0,0&gt;</tt> and <tt>&lt;2,0,0,0&gt;</tt>,
	  which obviously is not what we want for our simulation runs.
	  For now we recommend <tt>-mapfile TXYZ</tt> which fills up a CN before going to the next CN so that MPI processes working on adjacent subdomains
	  are placed closely in the 3-D torus.
     </li>

     <li> <tt>&lt;mapfile&gt;</tt> is the name of a mapfile in which the distribution of the tasks is specified:
     It contains a line of <tt>x y z t</tt> coordinates for each MPI process.
     See sec. 6. of the "Best-Practise guide" mentioned above for an example and which LoadLeveler keywords to use.
     </li>
     </ul>
     <!-- TODO: Some notes about "topology aware placing of MPI processes" might become relevant in the future. -->
</li>

<li> If ug4 was dynamically linked add to the <tt>mpirun</tt> parameters <tt>-env LD_LIBRARY_PATH==/bgsys/drivers/ppcfloor/comm/lib/</tt>.
     Note: This parameter is (obviously) not necessary for completely static builds!
</li>

<li> <b>"Modules"</b> can be loaded by executing the <tt>module</tt> command, e.g. <tt>module load lapack</tt> for LAPACK.

     <!-- TODO: Nevertheless CMake says "Info: Not using Lapack. No package found"!? -->

     This is also for loading <b>performance analysis tools</b>, e.g. <tt>module load scalasca</tt>, <tt>module load UNITE</tt> etc.
     
     <!-- !!!TODO!!!: please provide a link which explains a bit more -->
     
     <!-- - no experience with this stuff yet! -->
</li>

<li> <a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUGENE/UserInfo/llview.html"><tt>llview</tt></a>
     is a <b>tool with a graphical X11 user interface</b> for displaying system status, running jobs,
     scheduling and prediction of the start time of jobs.

     <!-- TODO: --> 
     If estimated start times can not determined by <tt>llq -s &lt;job-id&gt;</tt> (see below) this is to our knowledge the only possibility to get this information.
</li>
		
<li> <b>Interactive jobs</b> can be started with the <a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUGENE/UserInfo/LoadLevelerInteractive.html"> llrun </a> command.
	
     <b>Example</b>:
     \verbatim
     llrun -np <NP> -exe ./ugshell -mode VN -mapfile TXYZ -verbose 2 -env LD_LIBRARY_PATH=/bgsys/drivers/ppcfloor/comm/lib/ $UGARGS
     \endverbatim

     Please note that <tt>llrun</tt> only allows jobs up to 256 (<tt>-mode SMP</tt>) / 512 (<tt>-mode DUAL</tt>) / 1024 (<tt>-mode VN</tt>) MPI processes!
</li>

<li> <b>Batch Jobs</b> are defined in so called <b>"LoadLeveler scripts"</b> and submitted
     by the <a href="http://www2.fz-juelich.de/jsc/jugene/usage/loadl/llsubmit/"><tt>llsubmit</tt></a> command
     to the "IBM Tivoli Workload Scheduler LoadLeveler" (TWS LoadLeveler),
     typically in the directory where the ug4 executable resides:

     \verbatim
     llsubmit <cmdfile>
     \endverbatim

     <tt>&lt;cmdfile&gt;</tt> is a (plain Unix) shell script file (i.e., the "LoadLeveler script"), which contains job definitions
     given by <b>"LoadLeveler keywords"</b> (some important examples are explained below).

     If <tt>llsubmit</tt> was able to submit the job it outputs a <b>job name</b> (e.g. <tt>jugene4b.zam.kfa-juelich.de.298508</tt>)
     with which a specific job can be identified in further commands, e.g. to cancel it (see below).

     The <b>output</b> of the run (messages from Frontend end Backend MPI - and the output of ug4 - is written to
     a file in the directory where <tt>llsubmit</tt> was executed and whose name begins with the job name you have specified
     in the LoadLeveler script and ends with <tt>&lt;job number&gt;.out</tt>.

     <p>
     For some <b>example LoadLeveler scripts</b> used with ug4 see subdirectory <tt>scripts/shell/</tt>:

     <ul>
     <li>
	<tt>ll_scale_gmg.x</tt> contains job definitions for a complete scalability study for GMG in 2-D and 3-D.
     </li>
     <li>
	<tt>ll_template.x</tt> also contains some documentation of LoadLeveler and mpirun parameters.
     </li>
     </ul>
     (All <tt>mpirun</tt> commands therein are commented out - to performe a specific run remove the comment sign.)

     Please change in your copy of one of these scripts at least the value of the </tt>notify_user</tt> keyword before submitting a job ...

     See also this <a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUGENE/UserInfo/LoadLeveler.html">more recent JSC documentation</a>,
     and especially the
     <a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUGENE/UserInfo/LoadLevelerSamples.html">Job File Samples</a>
     for more details.

<li> <b>LoadLeveler keywords</b> are strings embedded in comments beginning with the characters "# @".
	<!-- TODO: isn't there a better overview on the web? -->

     Some selected keywords:
     <ul>
     <li> <tt>job_type = bluegene</tt> specifies that the job is running on JuGene's CNs.
     <li> <tt>job_name</tt> specifies the name of the job, which will get part of the name of the output file.
     </li>

     <li> <tt>bg_size</tt> specifies the size of the BG/P partition reserved for the job in <b>number of compute nodes</b>.

     	  That is, for <tt>&lt;NP&gt;</tt> MPI processes, <tt>bg_size</tt> must be >= <tt>(&lt;NP&gt;)/4</tt>.

     	  Alternatively the size of a job can be defined by the <tt>bg_shape</tt> keyword.

	  See comments in the example LoadLeveler scripts for proper settings.
     </li>

     <li> <tt>bg_connection</tt> specifies the "connection type", i.e. the <b>network topology</b> used by a job.

     	  Connection type can be one in <tt>[TORUS| MESH | PREFER_TORUS]</tt>.
     	  Default is <tt>bg_connection = MESH</tt>.

	  <tt>bg_connection = TORUS</tt> - utilising the 3-D torus network - is the preferred topology for our jobs.
     	  For this <tt>bg_size</tt> (see above) must be >= 512.

     	  See also comments and usage in the example LoadLeveler scripts for proper settings.
     </li>
     </ul>
     Please note that keywords must not be followed by comments in the same line! 

     A nice introduction to LoadLeveler command file syntax is given e.g. <a href="https://docs.loni.org/wiki/LoadLeveler_Command_File_Syntax">here</a>.

<li> <tt>llq</tt> is used to display the status of jobs (of a specified user) in the queue/executed:
     \verbatim
     llq [-u  <userid>]
     \endverbatim

     <!-- TODO (to test): -->
     The estimated start time of a job can (maybe) also be determined by <tt>llq -s &lt;job-id&gt;-id</tt> (cf. https://docs.loni.org/wiki/Useful_LoadLeveler_Commands).
</li>

<li> <tt>llcancel</tt> is used to cancel a job (<tt>&lt;jobname&gt;</tt> as displayed by <tt>llq</tt>):
     \verbatim
     llcancel <jobname>
     \endverbatim
</li>

<li> Debugging: See documentation to the tools <tt>/bgsys/drivers/ppcfloor/tools/coreprocessor</tt>, <tt>gdbserver</tt> ...
</li>

<li> Available file systems. See <a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUDGE/Userinfo/Access_Environment.html">JSC documentation</a>
     for more details.
</li>

<li> Querying <b>Quota Status</b>:

     \verbatim
     q_cpuquota <options>
     \endverbatim

     Useful options: 
     \arg <tt>-?</tt>                   usage information and all options .
     \arg <tt>-j &lt;jobstepid&gt;</tt> for a single job.
     \arg <tt>-t &lt;time&gt;</tt>      for all jobs in the specified time, e.g.
          <tt>q_cpuquota -t 23.11.2011 01.12.2011</tt>.
     \arg <tt>-d &lt;number&gt;</tt> for last number of days (positive integer).
</li>

</ul>
	

<hr>
\subsubsection secVery_large_jobs_on_JuGene Very large jobs on JuGene
<hr>

<ul>
<li>Although its possible on JuGene to create shared libraries and run dynamically linked executables
    this is <b>in general not recommended</b>, since loading of shared libraries can delay the startup of such an application considerably,
    especially when using large partitions (8 racks or more).
    See also <a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUGENE/UserInfo/SharedLibraries.html">Shared Libraries and Dynamic Executables</a>.


    So, for <b>very large jobs</b> be sure to have ug4 built as a <b>completely static executable</b> (c.f. \ref secConfiguration_of_ug4_for_JuGene)
    since otherwise loading of the shared libraries consumes too much wall time!
</li>

<li><b>Very large jobs</b> (e.g. jobs larger than 32 racks) normally run on <b>Tuesday</b> only.

    Exceptions to this rule are possible in urgent cases (please contact the SC Support under
    <a href="sc@fz-juelich.de">sc@fz-juelich.de</a>).
</li>
</ul>

<p>
More information <a href="mailto:Ingo.Heppner@gcsc.uni-frankfurt.de">Ingo.Heppner@gcsc.uni-frankfurt.de</a>


<hr>
\section secHermit Hermit
<hr>
\subsection secHermit_Architecture Architecture
The <a href="https://wickie.hlrs.de/platforms/index.php/Cray_XE6">Cray XE6 ("Hermit")</a>. In Installation step 1, the XE6 is a 3552 node cluster. Each node is
a Dual Socket <a href="http://en.wikipedia.org/wiki/List_of_AMD_Opteron_microprocessors#Opteron_6200-series_.22Interlagos.22_.2832_nm.29">AMD Opteron 6276 (Interlagos)</a> @ 2.3GHz 16 cores each, which results in 113.664 cores in total. Normal nodes have 32 GB RAM, 480 special nodes have 64 GB (total 126 TB). (<a href="https://wickie.hlrs.de/platforms/index.php/CRAY_XE6_Hardware_and_Architecture"> Architecture</a>)
That is 1 GB RAM for each process when running the maximum of 32 processes on a node. 
Current maximum number of cores for one job is 64000. Speak to the administration for more nodes.

\subsection secHermit_General General 
Using the <a href="https://wickie.hlrs.de/platforms/index.php/CRAY_XE6_Using_the_Batch_System"> Batch system </a> and the <a href="https://wickie.hlrs.de/platforms/index.php/Workspace_mechanism"> Workspace mechanism </a>.
<br>
The Job Scheduler on Hermit is supported by ugsubmit/uginfo/ugcancel (\ref pageugsubmit). You might want to use -Hermit-workspace . <br>
Note that you have to choose modules every time you log in (you might want to add your <tt>module load/swap</tt> commands into your <tt>.bashrc</tt> or similar).

\subsection secHermit_GCC GCC

First, look what modules are loaded
\verbatim
module list
\endverbatim

There is one which is named PrgEnv-cray or PrgEnv-*. Now you swap that to PrgEnv-gnu:
\verbatim
module swap PrgEnv-cray PrgEnv-gnu
\endverbatim

There's a one-liner for this task:
\verbatim
module swap $(module li 2>&1 | awk '/PrgEnv/{print $2}') PrgEnv-gnu
\endverbatim

Then you start cmake with a \ref secCMake_ToolchainFiles :
\verbatim
cmake -DCMAKE_TOOLCHAIN_FILE=../toolchain_file_hermit_gcc.cmake ..
\endverbatim

\subsection secHermit_Cray Cray CC
The Cray C Compiler is not working at the moment because there is an internal compiler error in release mode.

Toolchain file is ../toolchain_file_hermit.cmake, and the module is PrgEnv-cray.
\verbatim
module swap $(module li 2>&1 | awk '/PrgEnv/{print $2}') PrgEnv-cray
cmake -DCMAKE_TOOLCHAIN_FILE=../toolchain_file_hermit.cmake ..
\endverbatim

\subsection secHermit_Workspace Workspace Mechanism
Access to the user file system (anything in your home directory) from your running job is very slow on Hermit.
It is very noticable in runs with 1024 cores and even if you are accessing only small files once like your script files.
The <a href="https://wickie.hlrs.de/platforms/index.php/Workspace_mechanism"> Workspace Mechanism </a> lets you create
a directory on a specialized parallel file system.
Use a script like this to create a workspace directory and copy all you data there:
\verbatim
runDir=`ws_allocate $mydate 15`
echo "Hermit Workspace dir is $runDir"
ln -s $runDir $(date "+workspace.%m-%d-%H%M");
cp -r ../scripts $runDir
cp -r ../data $runDir
\endverbatim

Now before you run UG4, you want UG to know about its new root dir. 
Be aware that also file written to can be damaged if you are not using the Workspace mechanism. 
See also the "-dir" and "-Hermit-workspace" option in \ref pageugsubmit.

\verbatim
export UG4_ROOT=$runDir
\endverbatim
Otherwise, UG4 will look in ../scripts/ for scripts and ../data/ for data, relative to the path of the binary.

<hr>
\section secSSH SSH
<hr>
\subsection secSSHKeys Exchanging SSH Keys
You can exchange RSA keys so you don't have to type in your password every time you connect.

<ul> 
<li> generate local ssh key (empty password):
\verbatim
ssh-keygen -b 2048 -t rsa
\endverbatim
<li> copy local ssh key to cluster.de (user mrupp):
\verbatim
cat .ssh/id_rsa.pub | ssh mrupp@cluster.de 'umask 077; cat >> .ssh/authorized_keys'
\endverbatim
</ul>

\subsection secSSHHoping SSH Hoping
Many supercomputers are only accessible by some fixed IP you specify. Since at home we do not have a fixed IP, you have 
to use SSH hoping. Say speedo's IP (speedo.gcsc.uni-frankfurt.de) is specified as the connecting IP, and your
account is mrupp@speedo.gcsc.uni-frankfurt.de, and rupp@supercomputer.de is the supercomputer account. Then you can 
connect to speedo via ssh, and from there further with ssh to the supercomputer:
\verbatim
ssh mrupp@speedo.gcsc.uni-frankfurt.de
ssh rupp@supercomputer.de
\endverbatim

There are two ways to speed this up
<ul>
<li>
hopping in one line:
\verbatim
ssh -t mrupp@speedo.gcsc.uni-frankfurt.de ssh rupp@supercomputer.de
\endverbatim

<li>
automatic hopping: Add the following lines to the file <tt>~/.ssh/config</tt>
\verbatim
Host supercomputer.de
User rupp
ForwardAgent yes
ProxyCommand ssh mrupp@speedo.gcsc.uni-frankfurt.de nc %h %p
\endverbatim
now you'll only have to enter
\verbatim
ssh supercomputer.de
\endverbatim
and enter the passwords.
</ul>


\subsection secSSHFS FUSE/SSHFS

Manipulating files on remote system can be really tedious.  
Fortunately, with SSHFS you can mount filesystems over ssh into your computer. 
It's like having a supercomputer as a USB stick. For that

<ul>
<li> MacOS <= 10.6: download <a href="http://code.google.com/p/macfuse/downloads/list">macfuse</a>. install.
MacOS >= 10.7: use OSX Fuse.

<li> install sshfs:
\verbatim
cd ~/Downloads/
svn co http://macfuse.googlecode.com/svn/trunk/filesystems/sshfs/binary sshfs-binaries
cd sshfs-binaries/
sudo cp sshfs-static-leopard /usr/bin/sshfs
\endverbatim
For tiger, use <tt>sshfs-static-tiger</tt>.
</ul>


Now FUSE/SSHFS is installed. Now we'll mount the home directory of mrupp@supercomputer.de to 
~/sshvolumes/mrupp\@supercomputer .
\verbatim
mkdir ~/sshvolumes
mkdir ~/sshvolumes/mrupp@supercomputer
sshfs mrupp@supercomputer.de: ~/sshvolumes/mrupp@supercomputer -o auto_cache,reconnect,volname=mrupp@supercomputer
\endverbatim

unmounting is done via
\verbatim
umount ~/sshvolumes/mrupp@supercomputer
\endverbatim
Note: SSHFS also works together with \ref secSSHHoping and \ref secSSHKeys . For Windows, try <a href="http://dokan-dev.net/en/download/">Dokan library</a>.
<!-- TODO: Add install help for windows/linux -->

\subsection secSSH_Eclipse SSHFS + Eclipse
Note for Eclipse users: You can also import a project on your mounted filesystem. For that, create a C/C++ project and as base directory you use the ug4 base directory.
You'll then even get SVN info. You can also build on the remote server, but don't use the normal build command:
Right Mouse click on the project -> Properties -> C/C++ Build. Disable "use default build command", then enter at "Build Command" something like
\verbatim
ssh mrupp@supercomputer.de "cd ug4; cd release; make -j4"
\endverbatim
(You can do the same for run/debug settings). <br>
An improved version is
\verbatim
/bin/bash ${ProjDirPath}/scripts/shell/sshfsmake mrupp@supercomputer.de /Volumes/mrupp@supercomputer ug4/debug -j4 -k
\endverbatim
See ug4/scripts/shell/sshfsmake for details (basically you are calling ssh mrupp@supercomputer.de "cd ug4/debug; make -j4", and then substituting remote paths with local ones, so eclipse knows which files contain errors).
<br>
Since SSHFS is not as fast as your local drive, you might want to disable the most bandwith consuming things (at Project Preferences):
- C/C++ Build -> Discovery Options: Disable "Automated discovery of paths and symbols"
- C/C++ Build -> Logging: Disable Logging
- C/C++ General -> Indexer -> Disable Indexer

If that does not help, consider Eclipse -> Preferences -> Team -> SVN -> Performance , altough deactivating "deep outgoing state" is not recommended.<br>
Remember that Eclipse does not know that the files are not on your local machine, so when you are doing big SVN checkouts in Eclipse, all data is going to your computer, and then to the remote machine. 
Consider using the remote shell for those tasks. 

<hr>

*/
